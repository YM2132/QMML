{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:32.177401Z",
     "start_time": "2024-02-19T17:02:32.175399Z"
    }
   },
   "id": "d6f21b6fce87bf60"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable to enable CPU fallback for MPS\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:33.064349Z",
     "start_time": "2024-02-19T17:02:33.060452Z"
    }
   },
   "id": "76c2e0723644fa95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AlexNet - The paper which started the deep learning revolution!!!\n",
    "\n",
    "AlexNet is a seminal paper in the field of deep learning, it was one of the first deep neural networks trained in the wild on a real\n",
    "problem. ImageNet is the dataset this model trained on and the dataset is a classification task, with 1.2 million images and 1000 classes.\n",
    "\n",
    "When AlexNet was released it changed the computer vision game, shifting focus from handcrafted specific methods to more general CV \n",
    "methods, i.e. neural networks. It was 10.3% ahead in top-1 against the next best competitor (top-1 means of the classes how many did it\n",
    "get correct).\n",
    "\n",
    "In this session I will provide you with an AlexNet cookbook to help you implement the model.\n",
    "\n",
    "Link to the paper: https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9933e11ee9687c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boiler plate code blocks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c26c3983a2eab1ce"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:36.851310Z",
     "start_time": "2024-02-19T17:02:35.382885Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "\tdevice = torch.device(\"mps\")\n",
    "\tx = torch.ones(1, device=device)\n",
    "\tprint(x)\n",
    "    \n",
    "elif torch.backends.cuda.is_built():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tx = torch.ones(1, device=device)\n",
    "\tprint (x)\n",
    "else:\n",
    "\tdevice = None\n",
    "\tprint (\"MPS device not found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:37.012902Z",
     "start_time": "2024-02-19T17:02:36.870112Z"
    }
   },
   "id": "6830a59f993f70d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using the ImageNette dataset (a subset of ImageNet as ImageNet is around 125gb so it's not feasible to run without a GPU and a lot of time)\n",
    "\n",
    "To download ImageNette run these following commands:\n",
    "\n",
    "mkdir Data && cd data \n",
    "\n",
    "wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
    "\n",
    "tar -xvf imagenette2-160.tgz \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bfa2e296d1467a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The AlexNet CookBook\n",
    "<h2><b> For today's session I will provide you with a list of ingredients, which will be all the layers you need </b></h2>\n",
    "\n",
    "Beware some things are missing from this cookbook and you will need to utilise the paper to figure out the exact details.\n",
    "\n",
    "AlexNet Implementation Ingredients list:\n",
    "\n",
    "** n represents the number of total layers, so if n = 3 then there will be 3 of those layers **\n",
    "\n",
    "- Convolutional layers:\n",
    "\t- conv1 - 11x11 @ 96 \n",
    "\t- conv2 - 5x5 @ 256\n",
    "\t- conv3 - 3x3 @ 384\n",
    "\t- conv4 - 3x3 @ 384\n",
    "\t- conv5 - 3x3 @ 256\n",
    "\n",
    "- Fully connected layers:\n",
    "\t- fc1 - in_features = ?, out_features = 4096\n",
    "\t- fc2 - in_features = 4096, out_features = 4096\n",
    "\t- fc3 - in_features = 4096, out_features = 1000\n",
    "\n",
    "- Non-Linear activation layers:\n",
    "\t- relu_n - ReLU layers follow every layer\n",
    "\n",
    "- Norm layers: \n",
    "\t- LRN_n - Follows specific layers, check the paper for more details or ask me :) - size=?, alpha=?, beta=?, k=?\n",
    "\n",
    "- maxpool layers:\n",
    "\t- mp_n - Once again follows specific layers, check the paper for more details or ask me :)\n",
    "\n",
    "And that is all you need to construct AlexNet!!!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2add9cd31bd56cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's go through some of the key building ingredients above and what they're all about!\n",
    "\n",
    "## ReLU:\n",
    "\t- Activation functions are needed in deep neural networks as they allow the network to learn non-linear functions. An activation function\n",
    "\ttakes the output of a linear tranformation and makes it non-linear.\n",
    "\t- ReLU took the place of sigmoid type activations due to the benefits it provides. Sigmoid is prone to saturation, becuase as the value of its input gets large it gets close to 1 or 0 resulting in a small gradient which does not allow the gradient to flow backwards through the network. ReLU prevents this by having 0 or 1 gradient when the unit is switched off or 1 whenever the unit is on (negative and positive input respectively) allowing the gradient to flow backwards.\n",
    "\t- ReLU is a piecewise function (consisting of multiple sub-functions) of two linear functions, this means it retains the nice properties of linear functions i.e. they are good at being optimised by gradient descent.\n",
    "\t- In a nutshell, ReLU is an activation function which allows neural networks to learn non-linear relationships and is better at this than sigmoid functions.\n",
    "\n",
    "## Local Response Normalization:\n",
    "\t- LRN implements an idea from neurobiology, which is that an excited neuron inhibits its neighbours creating contrast and increasing \n",
    "\tsensory perception. It does so for CNNs, essentially trying to place more importance on relatively high activation values.\n",
    "\n",
    "## Convolution layer (check lesson 12 for further details):\n",
    "\t- The main layer which performs the feature extraction.\n",
    "\t- Creates many filters which filter for different important aspects in the inputs (from edges and lines to more advanced artifacts)\n",
    "\n",
    "## Max pool layer:\n",
    "\t- Outputs a summarisation of an n x n map of a filter map (n x n is the kernel size).\n",
    "\t- This helps to create invariance to translation as we remove the dependecy on the precise spatial information.\n",
    "\t- It extracts most important information while discarding the precise spatial information, allowing us to classify the same thing\n",
    "\twhich may appear in different places.\n",
    "\t\n",
    "## Fully connected layer:\n",
    "\t- The goal of the fully connected layer is to bring together the local information learned by the convolution layers and integrate it together to allow the model to classify the full images.\n",
    "\t- They also add to the non-linear learning capability of the model (through the ReLU activation).\n",
    "\t- Further removes emphasis on spatial structure of input images. FC layers care more about whether a feature is present rather than its location.\n",
    "\t- Allow for nice output of the model, i.e. classes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ca07bacc819004e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.conv1 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(11, 11),\n",
    "\t\t\tin_channels=3,\n",
    "\t\t\tout_channels=96,\n",
    "\t\t\tstride=4,\n",
    "\t\t)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.LRN1 = nn.LocalResponseNorm(\n",
    "\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n",
    "\t\t)\n",
    "\t\tself.maxpool1 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=(2, 2),\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.conv2 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(5, 5),\n",
    "\t\t\tin_channels=96,\n",
    "\t\t\tout_channels=256,\n",
    "\t\t\tpadding=2,\n",
    "\t\t)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\t\tself.LRN2 = nn.LocalResponseNorm(\n",
    "\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n",
    "\t\t)\n",
    "\t\tself.maxpool2 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=2,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.conv3 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=256,\n",
    "\t\t\tout_channels=384,\n",
    "\t\t\tpadding=1,\n",
    "\t\t)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\t\t\n",
    "\t\tself.conv4 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=384,\n",
    "\t\t\tout_channels=384,\n",
    "\t\t\tpadding=1\n",
    "\t\t)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\t\t\n",
    "\t\tself.conv5 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=384,\n",
    "\t\t\tout_channels=256,\n",
    "\t\t\tpadding=1\n",
    "\t\t)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\t\tself.maxpool3 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=2\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.fc1 = nn.Linear(9216, 4096)\n",
    "\t\tself.relu6 = nn.ReLU()\n",
    "\t\tself.dropout1 = nn.Dropout(p=0.5)\n",
    "\t\t\n",
    "\t\tself.fc2 = nn.Linear(4096, 4096)\n",
    "\t\tself.relu7 = nn.ReLU()\n",
    "\t\tself.dropout2 = nn.Dropout(p=0.5)\n",
    "\t\t\n",
    "\t\tself.fc3 = nn.Linear(4096, 1000)\n",
    "\t\t#self.fc3 = nn.Linear(4096, 10)  # If using CIFAR10\n",
    "\t\t# self.relu8 = nn.ReLU() is this supposed to be here?\n",
    "\t\t#self.softmax = nn.Softmax()\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# This block represents first convolution\n",
    "\t\tx = self.conv1(x)\n",
    "\t\t#print(f'Shape after conv1: {x.shape}')\n",
    "\t\tx = self.relu1(x)\n",
    "\t\t#print(f'Shape after ReLU1: {x.shape}')\n",
    "\t\tx = self.LRN1(x)\n",
    "\t\t#print(f'Shape after LRN1: {x.shape}')\n",
    "\t\tx = self.maxpool1(x)\n",
    "\t\t#print(f'Shape after MP1: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Second convolution\n",
    "\t\tx = self.conv2(x)\n",
    "\t\t#print(f'Shape after conv2: {x.shape}')\n",
    "\t\tx = self.relu2(x)\n",
    "\t\t#print(f'Shape after ReLU2: {x.shape}')\n",
    "\t\tx = self.LRN2(x)\n",
    "\t\t#print(f'Shape after LRN2: {x.shape}')\n",
    "\t\tx = self.maxpool2(x)\n",
    "\t\t#print(f'Shape after MP2: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Third convolution\n",
    "\t\tx = self.conv3(x)\n",
    "\t\t#print(f'Shape after conv3: {x.shape}')\n",
    "\t\tx = self.relu3(x)\n",
    "\t\t#print(f'Shape after ReLU3: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Fourth convolution\n",
    "\t\tx = self.conv4(x)\n",
    "\t\t#print(f'Shape after conv4: {x.shape}')\n",
    "\t\tx = self.relu4(x)\n",
    "\t\t#print(f'Shape after relu4: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Fifth convolution\n",
    "\t\tx = self.conv5(x)\n",
    "\t\t#print(f'Shape after conv5: {x.shape}')\n",
    "\t\tx = self.relu5(x)\n",
    "\t\t#print(f'Shape after relu5: {x.shape}')\n",
    "\t\tx = self.maxpool3(x)\n",
    "\t\t#print(f'Shape after MP3: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Before passing to the fully connected layer we must flatten the tensor as nn.Linear expects a matrix (2d input)\n",
    "\t\tx = torch.flatten(x, start_dim=1)\n",
    "\t\t#print(x.shape)\n",
    "\t\t\n",
    "\t\t# First fully connected layer\n",
    "\t\tx = self.fc1(x)\n",
    "\t\t#print(f'Shape after fc1: {x.shape}')\n",
    "\t\tx = self.relu6(x)\n",
    "\t\t#print(f'Shape after relu6: {x.shape}')\n",
    "\t\tx = self.dropout1(x)\n",
    "\t\t#print(f'Shape after dropout1: {x.shape}\\n')\n",
    "\t\n",
    "\t\t# Second fully connected layer\n",
    "\t\tx = self.fc2(x)\n",
    "\t\t#print(f'Shape after fc2: {x.shape}')\n",
    "\t\tx = self.relu7(x)\n",
    "\t\t#print(f'Shape after relu7: {x.shape}')\n",
    "\t\tx = self.dropout2(x)\n",
    "\t\t#print(f'Shape after dropout2: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Final FC layer\n",
    "\t\tx = self.fc3(x)\n",
    "\t\t#print(f'Shape after fc3: {x.shape}')\n",
    "\t\t# x = self.relu8(x)\n",
    "\t\t#print(f'Shape after relu8: {x.shape}')\n",
    "\t\t#x = self.softmax(x)\n",
    "\t\t#print(f'Shape after softmax: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\treturn x\n",
    "\t\n",
    "model = AlexNet()\n",
    "\n",
    "if device is not None:\n",
    "\tmodel.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:40.026425Z",
     "start_time": "2024-02-19T17:02:39.527850Z"
    }
   },
   "id": "3d524cba87ac5e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((227, 227)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "TRAIN_DATA_DIR = './Data/imagenette2-160/train'\n",
    "TEST_DATA_DIR = './Data/imagenette2-160/val'\n",
    "\n",
    "# Load ImageNette dataset\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "        TRAIN_DATA_DIR, transform=transform\n",
    "    )\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "testset= torchvision.datasets.ImageFolder(\n",
    "        TEST_DATA_DIR, transform=transform\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:40.521083Z",
     "start_time": "2024-02-19T17:02:40.442557Z"
    }
   },
   "id": "964ea73014cde8bf"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/nn/functional.py:2592: UserWarning: The operator 'aten::avg_pool3d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] training loss: 5.611\n",
      "Epoch [1] validation loss: 2.841, accuracy: 10.06%\n",
      "LR: [0.005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/__init__.py\", line 1209, in <module>\n",
      "    from .storage import _StorageBase, TypedStorage, _LegacyStorage, UntypedStorage, _warn_typed_storage_removal\n",
      "  File \"/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/storage.py\", line 14, in <module>\n",
      "    import numpy as np\n",
      "  File \"/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/numpy/__init__.py\", line 191, in <module>\n",
      "    core.numerictypes.typeDict,\n",
      "    ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/numpy/core/__init__.py\", line 161, in __getattr__\n",
      "    raise AttributeError(f\"Module {__name__!r} has no attribute {name!r}\")\n",
      "AttributeError: Module 'numpy.core' has no attribute 'numerictypes'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()  \u001B[38;5;66;03m# Set model to training mode\u001B[39;00m\n\u001B[1;32m      9\u001B[0m running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader, \u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m     12\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     13\u001B[0m     inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/utils/data/dataloader.py:439\u001B[0m, in \u001B[0;36mDataLoader.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    437\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iterator\n\u001B[1;32m    438\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 439\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_iterator()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/utils/data/dataloader.py:387\u001B[0m, in \u001B[0;36mDataLoader._get_iterator\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_worker_number_rationality()\n\u001B[0;32m--> 387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _MultiProcessingDataLoaderIter(\u001B[38;5;28mself\u001B[39m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1040\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter.__init__\u001B[0;34m(self, loader)\u001B[0m\n\u001B[1;32m   1033\u001B[0m w\u001B[38;5;241m.\u001B[39mdaemon \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   1034\u001B[0m \u001B[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001B[39;00m\n\u001B[1;32m   1035\u001B[0m \u001B[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001B[39;00m\n\u001B[1;32m   1036\u001B[0m \u001B[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001B[39;00m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001B[39;00m\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;66;03m#     AssertionError: can only join a started process.\u001B[39;00m\n\u001B[0;32m-> 1040\u001B[0m w\u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_index_queues\u001B[38;5;241m.\u001B[39mappend(index_queue)\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_workers\u001B[38;5;241m.\u001B[39mappend(w)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/process.py:121\u001B[0m, in \u001B[0;36mBaseProcess.start\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _current_process\u001B[38;5;241m.\u001B[39m_config\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemon\u001B[39m\u001B[38;5;124m'\u001B[39m), \\\n\u001B[1;32m    119\u001B[0m        \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdaemonic processes are not allowed to have children\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    120\u001B[0m _cleanup()\n\u001B[0;32m--> 121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_Popen(\u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sentinel \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_popen\u001B[38;5;241m.\u001B[39msentinel\n\u001B[1;32m    123\u001B[0m \u001B[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001B[39;00m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;66;03m# reference to the process object (see bpo-30775)\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/context.py:224\u001B[0m, in \u001B[0;36mProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    222\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[0;32m--> 224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _default_context\u001B[38;5;241m.\u001B[39mget_context()\u001B[38;5;241m.\u001B[39mProcess\u001B[38;5;241m.\u001B[39m_Popen(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/context.py:288\u001B[0m, in \u001B[0;36mSpawnProcess._Popen\u001B[0;34m(process_obj)\u001B[0m\n\u001B[1;32m    285\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_Popen\u001B[39m(process_obj):\n\u001B[1;32m    287\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpopen_spawn_posix\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Popen\n\u001B[0;32m--> 288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Popen(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, process_obj):\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fds \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m---> 32\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/popen_fork.py:19\u001B[0m, in \u001B[0;36mPopen.__init__\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturncode \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfinalizer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_launch(process_obj)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001B[0m, in \u001B[0;36mPopen._launch\u001B[0;34m(self, process_obj)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msentinel \u001B[38;5;241m=\u001B[39m parent_r\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(parent_w, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwb\u001B[39m\u001B[38;5;124m'\u001B[39m, closefd\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m---> 62\u001B[0m         f\u001B[38;5;241m.\u001B[39mwrite(fp\u001B[38;5;241m.\u001B[39mgetbuffer())\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m     fds_to_close \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, weight_decay=0.0005, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "for epoch in range(35):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Assuming test_loader is used as a validation loader\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Update the LR scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    print(f'LR: {scheduler.get_last_lr()}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:07:37.883266Z",
     "start_time": "2024-02-19T17:02:41.821275Z"
    }
   },
   "id": "b23f9e9d64e1e7b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e56f8329f99c9ed1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
