{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTORCH_ENABLE_MPS_FALLBACK=1\n"
     ]
    }
   ],
   "source": [
    "%env PYTORCH_ENABLE_MPS_FALLBACK=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:32.177401Z",
     "start_time": "2024-02-19T17:02:32.175399Z"
    }
   },
   "id": "d6f21b6fce87bf60"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the environment variable to enable CPU fallback for MPS\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:33.064349Z",
     "start_time": "2024-02-19T17:02:33.060452Z"
    }
   },
   "id": "76c2e0723644fa95"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# AlexNet - The paper which started the deep learning revolution!!!\n",
    "\n",
    "AlexNet is a seminal paper in the field of deep learning, it was one of the first deep neural networks trained in the wild on a real\n",
    "problem. ImageNet is the dataset this model trained on and the dataset is a classification task, with 1.2 million images and 1000 classes.\n",
    "\n",
    "When AlexNet was released it changed the computer vision game, shifting focus from handcrafted specific methods to more general CV \n",
    "methods, i.e. neural networks. It was 10.3% ahead in top-1 against the next best competitor (top-1 means of the classes how many did it\n",
    "get correct).\n",
    "\n",
    "In this session I will provide you with an AlexNet cookbook to help you implement the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9933e11ee9687c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Boiler plate code blocks"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c26c3983a2eab1ce"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:36.851310Z",
     "start_time": "2024-02-19T17:02:35.382885Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "\tdevice = torch.device(\"mps\")\n",
    "\tx = torch.ones(1, device=device)\n",
    "\tprint(x)\n",
    "    \n",
    "elif torch.backends.cuda.is_built():\n",
    "\tdevice = torch.device(\"cuda\")\n",
    "\tx = torch.ones(1, device=device)\n",
    "\tprint (x)\n",
    "else:\n",
    "\tdevice = None\n",
    "\tprint (\"MPS device not found.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:37.012902Z",
     "start_time": "2024-02-19T17:02:36.870112Z"
    }
   },
   "id": "6830a59f993f70d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will be using the ImageNette dataset (a subset of ImageNet as ImageNet is around 125gb so it's not feasible to run without a GPU and a lot of time)\n",
    "\n",
    "To download ImageNette run these following commands:\n",
    "\n",
    "mkdir Data && cd data \n",
    "wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\n",
    "tar -xvf imagenette2-160.tgz \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8bfa2e296d1467a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The AlexNet CookBook\n",
    "<h2><b> For today's session I will provide you with a list of ingredients, which will be all the layers you need </b></h2>\n",
    "\n",
    "Beware some things are missing from this cookbook and you will need to utilise the paper to figure out the exact details.\n",
    "\n",
    "AlexNet Implementation Ingredients list:\n",
    "\n",
    "** n represents the number of total layers, so if n = 3 then there will be 3 of those layers **\n",
    "\n",
    "- Convolutional layers:\n",
    "\t- conv1 - 11x11 @ 96 \n",
    "\t- conv2 - 5x5 @ 256\n",
    "\t- conv3 - 3x3 @ 384\n",
    "\t- conv4 - 3x3 @ 384\n",
    "\t- conv5 - 3x3 @ 256\n",
    "\n",
    "- Fully connected layers:\n",
    "\t- fc1 - in_features = ?, out_features = 4096\n",
    "\t- fc2 - in_features = 4096, out_features = 4096\n",
    "\t- fc3 - in_features = 4096, out_features = 1000\n",
    "\n",
    "- Non-Linear activation layers:\n",
    "\t- relu_n - ReLU layers follow every layer\n",
    "\n",
    "- Norm layers: \n",
    "\t- LRN_n - Follows specific layers, check the paper for more details or ask me :) - size=?, alpha=?, beta=?, k=?\n",
    "\n",
    "- maxpool layers:\n",
    "\t- mp_n - Once again follows specific layers, check the paper for more details or ask me :)\n",
    "\n",
    "And that is all you need to construct AlexNet!!!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c2add9cd31bd56cb"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\tself.conv1 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(11, 11),\n",
    "\t\t\tin_channels=3,\n",
    "\t\t\tout_channels=96,\n",
    "\t\t\tstride=4,\n",
    "\t\t)\n",
    "\t\tself.relu1 = nn.ReLU()\n",
    "\t\tself.LRN1 = nn.LocalResponseNorm(\n",
    "\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n",
    "\t\t)\n",
    "\t\tself.maxpool1 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=(2, 2),\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.conv2 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(5, 5),\n",
    "\t\t\tin_channels=96,\n",
    "\t\t\tout_channels=256,\n",
    "\t\t\tpadding=2,\n",
    "\t\t)\n",
    "\t\tself.relu2 = nn.ReLU()\n",
    "\t\tself.LRN2 = nn.LocalResponseNorm(\n",
    "\t\t\tsize=5, alpha=10**(-4), beta=0.75, k=2\n",
    "\t\t)\n",
    "\t\tself.maxpool2 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=2,\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.conv3 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=256,\n",
    "\t\t\tout_channels=384,\n",
    "\t\t\tpadding=1,\n",
    "\t\t)\n",
    "\t\tself.relu3 = nn.ReLU()\n",
    "\t\t\n",
    "\t\tself.conv4 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=384,\n",
    "\t\t\tout_channels=384,\n",
    "\t\t\tpadding=1\n",
    "\t\t)\n",
    "\t\tself.relu4 = nn.ReLU()\n",
    "\t\t\n",
    "\t\tself.conv5 = nn.Conv2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tin_channels=384,\n",
    "\t\t\tout_channels=256,\n",
    "\t\t\tpadding=1\n",
    "\t\t)\n",
    "\t\tself.relu5 = nn.ReLU()\n",
    "\t\tself.maxpool3 = nn.MaxPool2d(\n",
    "\t\t\tkernel_size=(3, 3),\n",
    "\t\t\tstride=2\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\tself.fc1 = nn.Linear(9216, 4096)\n",
    "\t\tself.relu6 = nn.ReLU()\n",
    "\t\tself.dropout1 = nn.Dropout(p=0.5)\n",
    "\t\t\n",
    "\t\tself.fc2 = nn.Linear(4096, 4096)\n",
    "\t\tself.relu7 = nn.ReLU()\n",
    "\t\tself.dropout2 = nn.Dropout(p=0.5)\n",
    "\t\t\n",
    "\t\tself.fc3 = nn.Linear(4096, 1000)\n",
    "\t\t#self.fc3 = nn.Linear(4096, 10)  # If using CIFAR10\n",
    "\t\t# self.relu8 = nn.ReLU() is this supposed to be here?\n",
    "\t\t#self.softmax = nn.Softmax()\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# This block represents first convolution\n",
    "\t\tx = self.conv1(x)\n",
    "\t\t#print(f'Shape after conv1: {x.shape}')\n",
    "\t\tx = self.relu1(x)\n",
    "\t\t#print(f'Shape after ReLU1: {x.shape}')\n",
    "\t\tx = self.LRN1(x)\n",
    "\t\t#print(f'Shape after LRN1: {x.shape}')\n",
    "\t\tx = self.maxpool1(x)\n",
    "\t\t#print(f'Shape after MP1: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Second convolution\n",
    "\t\tx = self.conv2(x)\n",
    "\t\t#print(f'Shape after conv2: {x.shape}')\n",
    "\t\tx = self.relu2(x)\n",
    "\t\t#print(f'Shape after ReLU2: {x.shape}')\n",
    "\t\tx = self.LRN2(x)\n",
    "\t\t#print(f'Shape after LRN2: {x.shape}')\n",
    "\t\tx = self.maxpool2(x)\n",
    "\t\t#print(f'Shape after MP2: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Third convolution\n",
    "\t\tx = self.conv3(x)\n",
    "\t\t#print(f'Shape after conv3: {x.shape}')\n",
    "\t\tx = self.relu3(x)\n",
    "\t\t#print(f'Shape after ReLU3: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Fourth convolution\n",
    "\t\tx = self.conv4(x)\n",
    "\t\t#print(f'Shape after conv4: {x.shape}')\n",
    "\t\tx = self.relu4(x)\n",
    "\t\t#print(f'Shape after relu4: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Fifth convolution\n",
    "\t\tx = self.conv5(x)\n",
    "\t\t#print(f'Shape after conv5: {x.shape}')\n",
    "\t\tx = self.relu5(x)\n",
    "\t\t#print(f'Shape after relu5: {x.shape}')\n",
    "\t\tx = self.maxpool3(x)\n",
    "\t\t#print(f'Shape after MP3: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Before passing to the fully connected layer we must flatten the tensor as nn.Linear expects a matrix (2d input)\n",
    "\t\tx = torch.flatten(x, start_dim=1)\n",
    "\t\t#print(x.shape)\n",
    "\t\t\n",
    "\t\t# First fully connected layer\n",
    "\t\tx = self.fc1(x)\n",
    "\t\t#print(f'Shape after fc1: {x.shape}')\n",
    "\t\tx = self.relu6(x)\n",
    "\t\t#print(f'Shape after relu6: {x.shape}')\n",
    "\t\tx = self.dropout1(x)\n",
    "\t\t#print(f'Shape after dropout1: {x.shape}\\n')\n",
    "\t\n",
    "\t\t# Second fully connected layer\n",
    "\t\tx = self.fc2(x)\n",
    "\t\t#print(f'Shape after fc2: {x.shape}')\n",
    "\t\tx = self.relu7(x)\n",
    "\t\t#print(f'Shape after relu7: {x.shape}')\n",
    "\t\tx = self.dropout2(x)\n",
    "\t\t#print(f'Shape after dropout2: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\t# Final FC layer\n",
    "\t\tx = self.fc3(x)\n",
    "\t\t#print(f'Shape after fc3: {x.shape}')\n",
    "\t\t# x = self.relu8(x)\n",
    "\t\t#print(f'Shape after relu8: {x.shape}')\n",
    "\t\t#x = self.softmax(x)\n",
    "\t\t#print(f'Shape after softmax: {x.shape}\\n')\n",
    "\t\t\n",
    "\t\treturn x\n",
    "\t\n",
    "model = AlexNet()\n",
    "\n",
    "if device is not None:\n",
    "\tmodel.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:40.026425Z",
     "start_time": "2024-02-19T17:02:39.527850Z"
    }
   },
   "id": "3d524cba87ac5e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose(\n",
    "    [transforms.Resize((227, 227)),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "TRAIN_DATA_DIR = './Data/imagenette2-160/train'\n",
    "TEST_DATA_DIR = './Data/imagenette2-160/val'\n",
    "\n",
    "# Load ImageNette dataset\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "        TRAIN_DATA_DIR, transform=transform\n",
    "    )\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "testset= torchvision.datasets.ImageFolder(\n",
    "        TEST_DATA_DIR, transform=transform\n",
    "    )\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-19T17:02:40.521083Z",
     "start_time": "2024-02-19T17:02:40.442557Z"
    }
   },
   "id": "964ea73014cde8bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Karpathy_NN_course/lib/python3.11/site-packages/torch/nn/functional.py:2592: UserWarning: The operator 'aten::avg_pool3d.out' is not currently supported on the MPS backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/mps/MPSFallback.mm:13.)\n",
      "  div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, weight_decay=0.0005, momentum=0.9)\n",
    "\n",
    "# Define learning rate scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "for epoch in range(35):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch + 1}] training loss: {train_loss:.3f}')\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_running_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:  # Assuming test_loader is used as a validation loader\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_running_loss / len(test_loader)\n",
    "    val_accuracy = 100 * val_correct / val_total\n",
    "    print(f'Epoch [{epoch + 1}] validation loss: {val_loss:.3f}, accuracy: {val_accuracy:.2f}%')\n",
    "    \n",
    "    # Update the LR scheduler with validation loss\n",
    "    scheduler.step(val_loss)\n",
    "    print(f'LR: {scheduler.get_last_lr()}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-19T17:02:41.821275Z"
    }
   },
   "id": "b23f9e9d64e1e7b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e56f8329f99c9ed1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
